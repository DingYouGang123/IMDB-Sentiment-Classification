import torch
import numpy as np
import optuna
from optuna.integration.pytorch_lightning import PyTorchLightningPruningCallback
from datasets import load_dataset, DatasetDict
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    Trainer, TrainingArguments, EarlyStoppingCallback
)
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import os
import swanlab
from swanlab.integration.transformers import SwanLabCallback

# ========== è°ƒä¼˜é…ç½® ==========
TUNE_CONFIG = {
    "model_name": "roberta-base",  # é€‰å®šè°ƒä¼˜æ¨¡å‹
    "dataset": "imdb",
    "max_length": 512,
    "num_trials": 10,  # æœç´¢è½®æ•°
    "num_epochs": 3,  # æ¯è½®è°ƒä¼˜è®­ç»ƒè½®æ•°
    "output_dir": "./results/roberta-tuning",
    "swanlab_project": "PLM-IMDB-Tuning",
    "metric_for_best_model": "accuracy"  # è°ƒä¼˜ç›®æ ‡æŒ‡æ ‡
}

# åˆ›å»ºè°ƒä¼˜ç»“æœç›®å½•
os.makedirs(TUNE_CONFIG["output_dir"], exist_ok=True)

# ========== å¤ç”¨æ ¸å¿ƒå‡½æ•° ==========
def load_and_split_dataset() -> tuple:
    """åŠ è½½å¹¶åˆ†å‰²æ•°æ®é›†ï¼ˆä¸train_imdb.pyå®Œå…¨ä¸€è‡´ï¼‰"""
    dataset = load_dataset(TUNE_CONFIG["dataset"])
    test_dataset = dataset["test"]
    val_size = len(test_dataset) // 2
    val_dataset = test_dataset.select(range(val_size))
    test_dataset = test_dataset.select(range(val_size, len(test_dataset)))
    return dataset["train"], val_dataset, test_dataset

def compute_metrics(eval_pred):
    """è¯„ä¼°æŒ‡æ ‡è®¡ç®—ï¼ˆä¸train_imdb.pyå®Œå…¨ä¸€è‡´ï¼‰"""
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return {
        "accuracy": round(accuracy_score(labels, predictions), 4),
        "precision": round(precision_score(labels, predictions, average="binary"), 4),
        "recall": round(recall_score(labels, predictions, average="binary"), 4),
        "f1": round(f1_score(labels, predictions, average="binary"), 4)
    }

# ========== è°ƒä¼˜ç›®æ ‡å‡½æ•° ==========
def objective(trial: optuna.Trial) -> float:
    """Optunaç›®æ ‡å‡½æ•°ï¼šæœç´¢æœ€ä¼˜è¶…å‚æ•°å¹¶è¿”å›éªŒè¯é›†å‡†ç¡®ç‡"""
    # 1. å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´
    hyperparameters = {
        "learning_rate": trial.suggest_categorical(
            "learning_rate", [1e-5, 2e-5, 5e-5, 1e-4]  # å­¦ä¹ ç‡èŒƒå›´
        ),
        "per_device_train_batch_size": trial.suggest_categorical(
            "per_device_train_batch_size", [4, 8, 16]  # é€‚é…GPUæ˜¾å­˜
        ),
        "weight_decay": trial.suggest_categorical(
            "weight_decay", [0.01, 0.05, 0.1, 0.2]  # æ­£åˆ™åŒ–å¼ºåº¦ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        ),
        "warmup_ratio": trial.suggest_float(
            "warmup_ratio", 0.05, 0.2, step=0.05  # å­¦ä¹ ç‡é¢„çƒ­æ¯”ä¾‹ï¼Œç¨³å®šè®­ç»ƒåˆæœŸ
        )
    }

    # 2. åŠ è½½Tokenizerå’Œæ•°æ®é›†
    tokenizer = AutoTokenizer.from_pretrained(TUNE_CONFIG["model_name"])
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    train_dataset, val_dataset, _ = load_and_split_dataset()
    def tokenize_fn(batch):
        return tokenizer(batch["text"], padding="max_length", truncation=True, max_length=TUNE_CONFIG["max_length"])
    
    tokenized_train = train_dataset.map(tokenize_fn, batched=True)
    tokenized_val = val_dataset.map(tokenize_fn, batched=True)
    
    # æ ¼å¼åŒ–æ•°æ®é›†
    for ds in [tokenized_train, tokenized_val]:
        ds = ds.rename_column("label", "labels")
        ds.set_format("torch", columns=["input_ids", "attention_mask", "labels"])

    # 3. åŠ è½½æ¨¡å‹
    model = AutoModelForSequenceClassification.from_pretrained(
        TUNE_CONFIG["model_name"], num_labels=2
    )

    # 4. é…ç½®è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=os.path.join(TUNE_CONFIG["output_dir"], f"trial-{trial.number}"),
        eval_strategy="epoch",
        save_strategy="epoch",
        learning_rate=hyperparameters["learning_rate"],
        per_device_train_batch_size=hyperparameters["per_device_train_batch_size"],
        per_device_eval_batch_size=16,
        num_train_epochs=TUNE_CONFIG["num_epochs"],
        weight_decay=hyperparameters["weight_decay"],
        warmup_ratio=hyperparameters["warmup_ratio"],
        logging_steps=50,
        report_to="none",
        load_best_model_at_end=True,
        metric_for_best_model=TUNE_CONFIG["metric_for_best_model"],
        save_total_limit=1,
        disable_tqdm=True  # ç¦ç”¨è¿›åº¦æ¡ï¼Œå‡å°‘æ—¥å¿—å†—ä½™
    )

    # 5. å›è°ƒå‡½æ•°ï¼ˆæ—©åœ+å®éªŒè·Ÿè¸ª+å‰ªæï¼‰
    callbacks = [
        # æ—©åœï¼šéªŒè¯é›†æ€§èƒ½3è½®æ— æå‡åˆ™åœæ­¢è®­ç»ƒ
        EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.001),
        # SwanLabï¼šè®°å½•æ¯è½®è°ƒä¼˜çš„è¶…å‚æ•°å’Œæ€§èƒ½
        SwanLabCallback(
            project=TUNE_CONFIG["swanlab_project"],
            experiment_name=f"RoBERTa-Trial-{trial.number}",
            config={**TUNE_CONFIG, **hyperparameters},
            tags=["hyperparameter-tuning", "RoBERTa", "sentiment-analysis"]
        ),
        # Optunaå‰ªæï¼šæ€§èƒ½ä¸ä½³çš„å®éªŒæå‰ç»ˆæ­¢ï¼ŒèŠ‚çœèµ„æº
        PyTorchLightningPruningCallback(trial, "eval_accuracy")
    ]

    # 6. è®­ç»ƒä¸è¯„ä¼°
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_val,
        compute_metrics=compute_metrics,
        callbacks=callbacks
    )

    trainer.train()
    val_results = trainer.evaluate()
    val_accuracy = val_results["eval_accuracy"]

    # è®°å½•å½“å‰å®éªŒçš„æ‰€æœ‰æŒ‡æ ‡
    swanlab.log({
        **hyperparameters,
        "val_accuracy": val_accuracy,
        "val_f1": val_results["eval_f1"],
        "val_precision": val_results["eval_precision"],
        "val_recall": val_results["eval_recall"]
    })

    return val_accuracy

# ========== è°ƒä¼˜ä¸»å‡½æ•° ==========
def main():
    print(f"ğŸš€ å¼€å§‹RoBERTaæ¨¡å‹è¶…å‚æ•°è°ƒä¼˜ï¼ˆå…±{len(TUNE_CONFIG['num_trials'])}è½®ï¼‰")
    print(f"ğŸ“Œ è°ƒä¼˜ç›®æ ‡ï¼šæœ€å¤§åŒ–{config['metric_for_best_model']}")
    print(f"ğŸ” æœç´¢è¶…å‚æ•°ï¼šå­¦ä¹ ç‡ã€æ‰¹é‡å¤§å°ã€æƒé‡è¡°å‡ã€é¢„çƒ­æ¯”ä¾‹")

    # 1. åˆå§‹åŒ–Optunaç ”ç©¶
    study = optuna.create_study(
        direction="maximize",  # æœ€å¤§åŒ–å‡†ç¡®ç‡
        pruner=optuna.pruners.MedianPruner(n_warmup_steps=50),  # å‰ªæç­–ç•¥
        study_name="RoBERTa-IMDB-Tuning"
    )

    # 2. å¯åŠ¨è¶…å‚æ•°æœç´¢
    study.optimize(
        objective,
        n_trials=TUNE_CONFIG["num_trials"],
        show_progress_bar=True,
        catch=(Exception,)  # æ•è·å¼‚å¸¸ï¼Œé¿å…å•è½®å¤±è´¥ç»ˆæ­¢æ•´ä¸ªè°ƒä¼˜
    )

    # 3. è¾“å‡ºè°ƒä¼˜ç»“æœ
    best_trial = study.best_trial
    print(f"\n{'='*80}")
    print(f"ğŸ‰ è°ƒä¼˜å®Œæˆï¼æœ€ä¼˜ç»“æœå¦‚ä¸‹ï¼š")
    print(f"{'='*80}")
    print(f"æœ€ä¼˜éªŒè¯é›†å‡†ç¡®ç‡ï¼š{best_trial.value:.4f}")
    print(f"æœ€ä¼˜è¶…å‚æ•°ç»„åˆï¼š")
    for key, value in best_trial.params.items():
        print(f"  - {key}: {value}")

    # 4. ä¿å­˜æœ€ä¼˜è¶…å‚æ•°åˆ°æ–‡ä»¶
    best_hparams_path = os.path.join(TUNE_CONFIG["output_dir"], "best_hyperparameters.json")
    import json
    with open(best_hparams_path, "w") as f:
        json.dump(best_trial.params, f, indent=4)
    print(f"\nğŸ“„ æœ€ä¼˜è¶…å‚æ•°å·²ä¿å­˜è‡³ï¼š{best_hparams_path}")

    # 5. ç”¨æœ€ä¼˜è¶…å‚æ•°åœ¨æµ‹è¯•é›†ä¸ŠéªŒè¯æœ€ç»ˆæ€§èƒ½
    print(f"\nğŸ”§ ç”¨æœ€ä¼˜è¶…å‚æ•°éªŒè¯æµ‹è¯•é›†æ€§èƒ½...")
    train_best_model(best_trial.params)

def train_best_model(best_hparams: dict):
    """ä½¿ç”¨æœ€ä¼˜è¶…å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹ï¼Œå¹¶åœ¨æµ‹è¯•é›†è¯„ä¼°"""
    # åŠ è½½æ•°æ®å’ŒTokenizer
    tokenizer = AutoTokenizer.from_pretrained(TUNE_CONFIG["model_name"])
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    train_dataset, val_dataset, test_dataset = load_and_split_dataset()
    def tokenize_fn(batch):
        return tokenizer(batch["text"], padding="max_length", truncation=True, max_length=TUNE_CONFIG["max_length"])
    
    tokenized_train = train_dataset.map(tokenize_fn, batched=True)
    tokenized_val = val_dataset.map(tokenize_fn, batched=True)
    tokenized_test = test_dataset.map(tokenize_fn, batched=True)
    
    for ds in [tokenized_train, tokenized_val, tokenized_test]:
        ds = ds.rename_column("label", "labels")
        ds.set_format("torch", columns=["input_ids", "attention_mask", "labels"])

    # åŠ è½½æ¨¡å‹
    model = AutoModelForSequenceClassification.from_pretrained(
        TUNE_CONFIG["model_name"], num_labels=2
    )

    # æœ€ä¼˜å‚æ•°è®­ç»ƒé…ç½®
    training_args = TrainingArguments(
        output_dir=os.path.join(TUNE_CONFIG["output_dir"], "best-model"),
        eval_strategy="epoch",
        save_strategy="epoch",
        learning_rate=best_hparams["learning_rate"],
        per_device_train_batch_size=best_hparams["per_device_train_batch_size"],
        per_device_eval_batch_size=16,
        num_train_epochs=TUNE_CONFIG["num_epochs"],
        weight_decay=best_hparams["weight_decay"],
        warmup_ratio=best_hparams["warmup_ratio"],
        logging_steps=50,
        report_to="none",
        load_best_model_at_end=True,
        metric_for_best_model=TUNE_CONFIG["metric_for_best_model"],
        save_total_limit=1
    )

    # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_val,
        compute_metrics=compute_metrics,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
    )

    trainer.train()
    test_results = trainer.evaluate(tokenized_test)

    # è¾“å‡ºæµ‹è¯•é›†æœ€ç»ˆæ€§èƒ½
    print(f"\nğŸ“Š æœ€ä¼˜æ¨¡å‹æµ‹è¯•é›†æ€§èƒ½ï¼š")
    print(f"  - å‡†ç¡®ç‡ï¼š{test_results['eval_accuracy']:.4f}")
    print(f"  - F1åˆ†æ•°ï¼š{test_results['eval_f1']:.4f}")
    print(f"  - ç²¾ç¡®ç‡ï¼š{test_results['eval_precision']:.4f}")
    print(f"  - å¬å›ç‡ï¼š{test_results['eval_recall']:.4f}")

    # ä¿å­˜æµ‹è¯•é›†ç»“æœ
    test_results_path = os.path.join(TUNE_CONFIG["output_dir"], "test_results.json")
    import json
    with open(test_results_path, "w") as f:
        json.dump(test_results, f, indent=4)
    print(f"\nğŸ“„ æµ‹è¯•é›†ç»“æœå·²ä¿å­˜è‡³ï¼š{test_results_path}")

if __name__ == "__main__":
    main()