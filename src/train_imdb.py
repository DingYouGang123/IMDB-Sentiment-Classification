import torch
import numpy as np
from datasets import load_dataset, DatasetDict
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification, 
    Trainer, TrainingArguments, DebertaV2Tokenizer
)
from swanlab.integration.transformers import SwanLabCallback
import swanlab
import time
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import os

# ========== é…ç½®é¡¹ ==========
CONFIG = {
    "models": {  # å¾…å¯¹æ¯”æ¨¡å‹åˆ—è¡¨
        "bert-base-uncased": "BERT",
        "roberta-base": "RoBERTa",
        "distilbert-base-uncased": "DistilBERT",
        "albert-base-v2": "ALBERT",
        "deberta-v3-base": "DeBERTa",
    },
    "dataset": "imdb",
    "max_length": 512,
    "learning_rate": 2e-5,
    "batch_size": 8,
    "epochs": 3,
    "weight_decay": 0.01,
    "logging_steps": 100,
    "output_dir": "./results",
    "swanlab_project": "PLM-IMDB-Comparison"
}

# åˆ›å»ºç»“æœä¿å­˜ç›®å½•
os.makedirs(CONFIG["output_dir"], exist_ok=True)

def load_and_split_dataset() -> tuple:
    """åŠ è½½IMDBæ•°æ®é›†å¹¶åˆ†å‰²ä¸ºè®­ç»ƒé›†/éªŒè¯é›†/æµ‹è¯•é›†"""
    try:
        dataset = load_dataset(CONFIG["dataset"])
        print(f"âœ… æˆåŠŸåŠ è½½{CONFIG['dataset']}æ•°æ®é›†ï¼Œå…±{len(dataset['train'])+len(dataset['test'])}æ¡æ ·æœ¬")
    except Exception as e:
        raise RuntimeError(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥ï¼š{e}") from e
    
    # åˆ†å‰²æµ‹è¯•é›†ä¸ºéªŒè¯é›†å’Œæµ‹è¯•é›†
    test_dataset = dataset["test"]
    val_size = len(test_dataset) // 2
    val_dataset = test_dataset.select(range(val_size))
    test_dataset = test_dataset.select(range(val_size, len(test_dataset)))
    
    # æ•´ç†æ•°æ®é›†
    dataset_split = DatasetDict({
        "train": dataset["train"],
        "val": val_dataset,
        "test": test_dataset
    })
    print(f"ğŸ“Š æ•°æ®é›†åˆ†å‰²å®Œæˆï¼šè®­ç»ƒé›†{len(dataset_split['train'])}æ¡ | éªŒè¯é›†{len(dataset_split['val'])}æ¡ | æµ‹è¯•é›†{len(dataset_split['test'])}æ¡")
    return dataset_split["train"], dataset_split["val"], dataset_split["test"]

def get_tokenizer(model_name: str):
    """æ ¹æ®æ¨¡å‹åç§°åŠ è½½å¯¹åº”çš„Tokenizerï¼Œå¤„ç†ç‰¹æ®Šæƒ…å†µï¼ˆå¦‚Pad Tokenç¼ºå¤±ï¼‰"""
    try:
        if "deberta" in model_name.lower():
            tokenizer = DebertaV2Tokenizer.from_pretrained(model_name)
        else:
            tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # å¤„ç†æ— Pad Tokençš„æƒ…å†µ
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            print(f"âš ï¸  æ¨¡å‹{model_name}æ— Pad Tokenï¼Œå·²ä½¿ç”¨EOS Tokenæ›¿ä»£")
        return tokenizer
    except Exception as e:
        raise RuntimeError(f"âŒ TokenizeråŠ è½½å¤±è´¥ï¼ˆæ¨¡å‹ï¼š{model_name}ï¼‰ï¼š{e}") from e

def tokenize_function(batch, tokenizer):
    """æ‰¹é‡åˆ†è¯å‡½æ•°"""
    return tokenizer(
        batch["text"],
        padding="max_length",  # å›ºå®šé•¿åº¦å¡«å……
        truncation=True,
        max_length=CONFIG["max_length"]
    )

def compute_metrics(eval_pred):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼šå‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°"""
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    
    return {
        "accuracy": round(accuracy_score(labels, predictions), 4),
        "precision": round(precision_score(labels, predictions, average="binary"), 4),
        "recall": round(recall_score(labels, predictions, average="binary"), 4),
        "f1": round(f1_score(labels, predictions, average="binary"), 4)
    }

def train_and_evaluate_model(
    model_name: str,
    model_display_name: str,
    train_dataset,
    val_dataset,
    test_dataset
) -> dict:
    """è®­ç»ƒå•ä¸ªæ¨¡å‹å¹¶è¿”å›æµ‹è¯•é›†ç»“æœ"""
    print(f"\n{'='*60}")
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼š{model_display_name}ï¼ˆæ¨¡å‹ï¼š{model_name}ï¼‰")
    print(f"{'='*60}")
    
    # è®¡æ—¶å¼€å§‹
    start_time = time.time()
    
    # 1. åŠ è½½Tokenizerå¹¶åˆ†è¯
    tokenizer = get_tokenizer(model_name)
    tokenized_train = train_dataset.map(
        lambda batch: tokenize_function(batch, tokenizer),
        batched=True,
        desc=f"åˆ†è¯ï¼š{model_display_name}è®­ç»ƒé›†"
    )
    tokenized_val = val_dataset.map(
        lambda batch: tokenize_function(batch, tokenizer),
        batched=True,
        desc=f"åˆ†è¯ï¼š{model_display_name}éªŒè¯é›†"
    )
    tokenized_test = test_dataset.map(
        lambda batch: tokenize_function(batch, tokenizer),
        batched=True,
        desc=f"åˆ†è¯ï¼š{model_display_name}æµ‹è¯•é›†"
    )
    
    # 2. æ ¼å¼åŒ–æ•°æ®é›†
    for ds in [tokenized_train, tokenized_val, tokenized_test]:
        ds = ds.rename_column("label", "labels")
        ds.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
    
    # 3. åŠ è½½æ¨¡å‹
    try:
        model = AutoModelForSequenceClassification.from_pretrained(
            model_name,
            num_labels=2  # äºŒåˆ†ç±»ä»»åŠ¡
        )
    except Exception as e:
        raise RuntimeError(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼ˆæ¨¡å‹ï¼š{model_name}ï¼‰ï¼š{e}") from e
    
    # 4. é…ç½®è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=os.path.join(CONFIG["output_dir"], model_name.replace("/", "-")),
        eval_strategy="epoch",  # æ¯ä¸ªepochè¯„ä¼°ä¸€æ¬¡
        save_strategy="epoch",  # æ¯ä¸ªepochä¿å­˜ä¸€æ¬¡æ¨¡å‹
        learning_rate=CONFIG["learning_rate"],
        per_device_train_batch_size=CONFIG["batch_size"],
        per_device_eval_batch_size=CONFIG["batch_size"],
        num_train_epochs=CONFIG["epochs"],
        weight_decay=CONFIG["weight_decay"],
        logging_steps=CONFIG["logging_steps"],
        report_to="none",  # ç¦ç”¨é»˜è®¤æ—¥å¿—å·¥å…·ï¼Œä½¿ç”¨SwanLab
        load_best_model_at_end=True,  # è®­ç»ƒç»“æŸåŠ è½½æœ€ä¼˜æ¨¡å‹
        metric_for_best_model="accuracy",  # ä»¥å‡†ç¡®ç‡ä¸ºæœ€ä¼˜æ¨¡å‹åˆ¤å®šæ ‡å‡†
        save_total_limit=1  # åªä¿å­˜æœ€ä¼˜æ¨¡å‹
    )
    
    # 5. é…ç½®SwanLabå›è°ƒ
    swanlab_callback = SwanLabCallback(
        project=CONFIG["swanlab_project"],
        experiment_name=f"{model_display_name}-IMDB",
        config={k: v for k, v in CONFIG.items() if k != "models"},  # è®°å½•å®éªŒé…ç½®
        tags=["sentiment-analysis", "pre-trained-model", model_display_name]
    )
    
    # 6. åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_val,
        callbacks=[swanlab_callback],
        compute_metrics=compute_metrics
    )
    
    # 7. å¼€å§‹è®­ç»ƒ
    trainer.train()
    
    # 8. æµ‹è¯•é›†è¯„ä¼°
    test_results = trainer.evaluate(tokenized_test)
    training_time = round(time.time() - start_time, 2)
    test_results["training_time"] = training_time
    
    # 9. æ‰“å°ç»“æœ
    print(f"\nâœ… {model_display_name}è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“ˆ æµ‹è¯•é›†ç»“æœï¼šå‡†ç¡®ç‡{test_results['eval_accuracy']} | F1{test_results['eval_f1']} | è®­ç»ƒæ—¶é—´{training_time}s")
    return {
        "model_name": model_display_name,
        "accuracy": test_results["eval_accuracy"],
        "precision": test_results["eval_precision"],
        "recall": test_results["eval_recall"],
        "f1": test_results["eval_f1"],
        "training_time": training_time
    }

def plot_results(results: list):
    """ç»˜åˆ¶æ¨¡å‹æ€§èƒ½å¯¹æ¯”å›¾ï¼ˆä¿å­˜åˆ°resultsç›®å½•ï¼‰"""
    models = [r["model_name"] for r in results]
    accuracies = [r["accuracy"] for r in results]
    f1_scores = [r["f1"] for r in results]
    training_times = [r["training_time"] for r in results]
    
    # åˆ›å»ºå­å›¾
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    
    # å­å›¾1ï¼šå‡†ç¡®ç‡ä¸F1åˆ†æ•°
    x = np.arange(len(models))
    width = 0.35
    ax1.bar(x - width/2, accuracies, width, label="å‡†ç¡®ç‡", color="#2E86AB")
    ax1.bar(x + width/2, f1_scores, width, label="F1åˆ†æ•°", color="#A23B72")
    ax1.set_xlabel("æ¨¡å‹")
    ax1.set_ylabel("åˆ†æ•°ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰")
    ax1.set_title("å„æ¨¡å‹åˆ†ç±»æ€§èƒ½å¯¹æ¯”")
    ax1.set_xticks(x)
    ax1.set_xticklabels(models)
    ax1.legend()
    ax1.grid(axis="y", alpha=0.3)
    
    # å­å›¾2ï¼šè®­ç»ƒæ—¶é—´
    ax2.bar(models, training_times, color="#F18F01", alpha=0.7)
    ax2.set_xlabel("æ¨¡å‹")
    ax2.set_ylabel("è®­ç»ƒæ—¶é—´ï¼ˆsï¼Œè¶Šä½è¶Šå¥½ï¼‰")
    ax2.set_title("å„æ¨¡å‹è®­ç»ƒæ•ˆç‡å¯¹æ¯”")
    ax2.grid(axis="y", alpha=0.3)
    
    # è°ƒæ•´å¸ƒå±€å¹¶ä¿å­˜
    plt.tight_layout()
    save_path = os.path.join(CONFIG["output_dir"], "model_comparison.png")
    plt.savefig(save_path, dpi=300, bbox_inches="tight")
    print(f"\nğŸ“Š æ€§èƒ½å¯¹æ¯”å›¾å·²ä¿å­˜è‡³ï¼š{save_path}")

def main():
    """ä¸»å‡½æ•°ï¼šåŠ è½½æ•°æ®â†’è®­ç»ƒæ‰€æœ‰æ¨¡å‹â†’è¯„ä¼°â†’å¯è§†åŒ–ç»“æœ"""
    # 1. åŠ è½½å¹¶åˆ†å‰²æ•°æ®é›†
    train_dataset, val_dataset, test_dataset = load_and_split_dataset()
    
    # 2. è®­ç»ƒæ‰€æœ‰æ¨¡å‹
    all_results = []
    for model_name, display_name in CONFIG["models"].items():
        try:
            result = train_and_evaluate_model(
                model_name, display_name, train_dataset, val_dataset, test_dataset
            )
            all_results.append(result)
        except Exception as e:
            print(f"\nâŒ {display_name}è®­ç»ƒå¤±è´¥ï¼š{e}")
            continue
    
    # 3. è¾“å‡ºæ±‡æ€»ç»“æœ
    print(f"\n{'='*80}")
    print("ğŸ“‹ æ‰€æœ‰æ¨¡å‹æ€§èƒ½æ±‡æ€»")
    print(f"{'='*80}")
    print(f"{'æ¨¡å‹':<10} {'å‡†ç¡®ç‡':<10} {'F1åˆ†æ•°':<10} {'è®­ç»ƒæ—¶é—´(s)':<15}")
    print("-"*80)
    for res in all_results:
        print(f"{res['model_name']:<10} {res['accuracy']:<10.4f} {res['f1']:<10.4f} {res['training_time']:<15.2f}")
    
    # 4. ç»˜åˆ¶å¯è§†åŒ–å›¾è¡¨
    if all_results:
        plot_results(all_results)
    
    # 5. ä¿å­˜ç»“æœåˆ°CSV
    import pandas as pd
    results_df = pd.DataFrame(all_results)
    results_df.to_csv(os.path.join(CONFIG["output_dir"], "model_results.csv"), index=False)
    print(f"\nğŸ“„ ç»“æœæ•°æ®å·²ä¿å­˜è‡³ï¼š{os.path.join(CONFIG['output_dir'], 'model_results.csv')}")

if __name__ == "__main__":
    main()